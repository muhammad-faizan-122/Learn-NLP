
# LLM Optimization Methods
LLM size model size can be reduced without compromising the performance, Purpose of optimization is to reduce the size of model, reduce the inference time. LLM can be optimize using following methods.
1. Pruning
2. distillation.
3. Quantization.
1. **Pruning**
Pruning involve the reduce the size the model by removing the unrequire by using finetune process, which involve the forward and backward propagation algorithms.
2. **Distillation**
Distillation use teacher-student concept. teacher is pre-trained model or base model which student model it reduce size model which need to reduced.
3. **Quantization**
Quantization involve the reducing the precision of model, which significantly reduce the size and inference time of LLM.

**Resource:** https://coursera.org/learn/generative-ai-with-llms/lecture/qojKp/model-optimizations-for-deployment